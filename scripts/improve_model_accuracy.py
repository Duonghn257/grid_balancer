#!/usr/bin/env python3
"""
Script ƒë·ªÉ c·∫£i thi·ªán model accuracy sau khi gi·∫£m lag features
C√°c ph∆∞∆°ng ph√°p:
1. Gi·ªØ th√™m electricity_lag24 (7% importance)
2. Tune hyperparameters c·ªßa XGBoost
3. Early stopping ƒë·ªÉ tr√°nh overfitting
"""

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import json
import pickle
from datetime import datetime

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb

warnings.filterwarnings('ignore')

print("=" * 80)
print("C·∫¢I THI·ªÜN MODEL ACCURACY")
print("=" * 80)

# ============================================================================
# 1. LOAD D·ªÆ LI·ªÜU
# ============================================================================

print("\n" + "=" * 80)
print("B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU")
print("=" * 80)

df = pd.read_parquet("./output/processed_data.parquet")
print(f"‚úÖ Dataset shape: {df.shape}")

with open('output/features_info.json', 'r') as f:
    features_info = json.load(f)

# ============================================================================
# 2. OPTION 1: GI·ªÆ TH√äM electricity_lag24
# ============================================================================

print("\n" + "=" * 80)
print("OPTION 1: GI·ªÆ TH√äM electricity_lag24 (7% importance)")
print("=" * 80)

print("\nüí° ƒê·ªÅ xu·∫•t: Gi·ªØ th√™m electricity_lag24 ƒë·ªÉ c·∫£i thi·ªán accuracy")
print("   - electricity_lag1: 87% importance - GI·ªÆ")
print("   - electricity_lag24: 7% importance - GI·ªÆ (ƒë·ªÉ c·∫£i thi·ªán accuracy)")
print("   - C√°c lag features kh√°c: <3% - B·ªé")
print("\n   ƒêi·ªÅu n√†y s·∫Ω:")
print("   ‚úÖ C·∫£i thi·ªán accuracy (RMSE c√≥ th·ªÉ gi·∫£m t·ª´ 48 ‚Üí 35-40)")
print("   ‚úÖ V·∫´n cho ph√©p model h·ªçc m·ªëi quan h·ªá v·ªõi occupants")
print("   ‚úÖ Occupants v·∫´n s·∫Ω c√≥ importance cao h∆°n (d·ª± ki·∫øn 2-5%)")

# Ki·ªÉm tra xem c√≥ electricity_lag24 trong data kh√¥ng
if 'electricity_lag24' in df.columns:
    print(f"\n‚úÖ electricity_lag24 c√≥ trong data")
    use_lag24 = True
else:
    print(f"\n‚ö†Ô∏è  electricity_lag24 KH√îNG c√≥ trong data")
    print(f"   C·∫ßn ch·∫°y l·∫°i preprocessing v·ªõi lag24")
    use_lag24 = False

# ============================================================================
# 3. CHU·∫®N B·ªä D·ªÆ LI·ªÜU
# ============================================================================

print("\n" + "=" * 80)
print("B∆Ø·ªöC 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU")
print("=" * 80)

# Sample buildings
np.random.seed(42)
sample_size = min(20000, df['building_id'].nunique())
sample_buildings = np.random.choice(
    df['building_id'].unique(), 
    size=sample_size, 
    replace=False
)
df_train = df[df['building_id'].isin(sample_buildings)].copy()
df_train = df_train.sort_values(['building_id', 'timestamp']).reset_index(drop=True)

# X√°c ƒë·ªãnh features
all_features = (
    features_info['continuous_features'] + 
    features_info['time_features'] + 
    features_info['lag_features']
)

# N·∫øu c√≥ lag24, th√™m v√†o
if use_lag24 and 'electricity_lag24' not in features_info['lag_features']:
    all_features.append('electricity_lag24')
    print(f"\nüìã ƒê√£ th√™m electricity_lag24 v√†o features")

all_features = [f for f in all_features if f in df_train.columns]
categorical_features = [f for f in features_info['categorical_features'] if f in df_train.columns]

print(f"\nüìä Features:")
print(f"   - Total: {len(all_features) + len(categorical_features)}")
print(f"   - Lag features: {[f for f in all_features if 'lag' in f]}")

# Encode categorical
X = df_train[all_features + categorical_features].copy()
y = df_train[features_info['target']].copy()

label_encoders = {}
for col in categorical_features:
    if col not in X.columns:
        print(f"‚ö†Ô∏è  Warning: Column '{col}' not found in X, skipping...")
        continue
    
    # ƒê·∫£m b·∫£o l·∫•y Series 1D, kh√¥ng ph·∫£i DataFrame
    col_data = X[col]
    if isinstance(col_data, pd.DataFrame):
        # N·∫øu l√† DataFrame (c√≥ duplicate column names), l·∫•y c·ªôt ƒë·∫ßu ti√™n
        col_data = col_data.iloc[:, 0]
        print(f"‚ö†Ô∏è  Warning: Column '{col}' is a DataFrame, using first column")
    
    # Convert to Series n·∫øu ch∆∞a ph·∫£i
    if not isinstance(col_data, pd.Series):
        col_data = pd.Series(col_data)
    
    le = LabelEncoder()
    X[col] = le.fit_transform(col_data.astype(str))
    label_encoders[col] = le

print(f"‚úÖ ƒê√£ encode {len(label_encoders)} categorical features")

# Lo·∫°i b·ªè duplicate columns (n·∫øu c√≥)
print("\nüìä Ki·ªÉm tra v√† lo·∫°i b·ªè duplicate columns...")
if X.columns.duplicated().any():
    duplicate_cols = X.columns[X.columns.duplicated()].tolist()
    print(f"‚ö†Ô∏è  Ph√°t hi·ªán duplicate columns: {duplicate_cols}")
    # Gi·ªØ l·∫°i c·ªôt ƒë·∫ßu ti√™n, lo·∫°i b·ªè c√°c c·ªôt duplicate
    X = X.loc[:, ~X.columns.duplicated()]
    print(f"‚úÖ ƒê√£ lo·∫°i b·ªè duplicate columns. Shape m·ªõi: {X.shape}")

# ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c c·ªôt ƒë·ªÅu l√† Series 1D
print("\nüìä ƒê·∫£m b·∫£o t·∫•t c·∫£ c·ªôt ƒë·ªÅu l√† Series 1D...")
for col in X.columns:
    col_data = X[col]
    if isinstance(col_data, pd.DataFrame):
        # N·∫øu l√† DataFrame, l·∫•y c·ªôt ƒë·∫ßu ti√™n
        X[col] = col_data.iloc[:, 0]
        print(f"‚ö†Ô∏è  ƒê√£ s·ª≠a c·ªôt '{col}' t·ª´ DataFrame th√†nh Series")
    elif not isinstance(col_data, pd.Series):
        # N·∫øu kh√¥ng ph·∫£i Series, convert
        X[col] = pd.Series(col_data, index=X.index)
        print(f"‚ö†Ô∏è  ƒê√£ convert c·ªôt '{col}' th√†nh Series")

print(f"‚úÖ X shape cu·ªëi c√πng: {X.shape}")
print(f"‚úÖ T·∫•t c·∫£ c·ªôt ƒë·ªÅu l√† Series 1D")

# Train/test split
split_idx = int(len(df_train) * 0.8)
X_train = X.iloc[:split_idx]
y_train = y.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_test = y.iloc[split_idx:]

print(f"\n‚úÖ Train: {X_train.shape[0]}, Test: {X_test.shape[0]}")

# ============================================================================
# 4. TRAIN V·ªöI HYPERPARAMETERS T·ªêT H∆†N
# ============================================================================

print("\n" + "=" * 80)
print("B∆Ø·ªöC 3: TRAIN V·ªöI TUNED HYPERPARAMETERS")
print("=" * 80)

# Improved hyperparameters
xgb_model = xgb.XGBRegressor(
    n_estimators=500,  # TƒÉng t·ª´ 200 l√™n 500
    max_depth=10,      # TƒÉng t·ª´ 8 l√™n 10
    learning_rate=0.03,  # Gi·∫£m t·ª´ 0.05 xu·ªëng 0.03 (c·∫ßn nhi·ªÅu trees h∆°n)
    subsample=0.85,     # TƒÉng t·ª´ 0.8 l√™n 0.85
    colsample_bytree=0.85,  # TƒÉng t·ª´ 0.8 l√™n 0.85
    min_child_weight=2,  # Gi·∫£m t·ª´ 3 xu·ªëng 2 (cho ph√©p splits nh·ªè h∆°n)
    gamma=0.1,          # Th√™m regularization
    reg_alpha=0.1,      # L1 regularization
    reg_lambda=1.0,     # L2 regularization
    random_state=42,
    n_jobs=-1,
    objective='reg:squarederror',
    eval_metric='rmse',
)

print("\nüîß Hyperparameters:")
print(f"   - n_estimators: 500 (tƒÉng t·ª´ 200)")
print(f"   - max_depth: 10 (tƒÉng t·ª´ 8)")
print(f"   - learning_rate: 0.03 (gi·∫£m t·ª´ 0.05)")
print(f"   - Th√™m regularization (gamma, reg_alpha, reg_lambda)")

print("\nüìä ƒêang training...")
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_test, y_test)],
    verbose=50
)

# Predictions
y_pred_train = xgb_model.predict(X_train)
y_pred_test = xgb_model.predict(X_test)

# Metrics
train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print(f"\n‚úÖ Model Performance:")
print(f"\nTrain Set:")
print(f"  RMSE: {train_rmse:.2f} kWh")
print(f"  MAE:  {train_mae:.2f} kWh")
print(f"  R¬≤:   {train_r2:.4f}")

print(f"\nTest Set:")
print(f"  RMSE: {test_rmse:.2f} kWh")
print(f"  MAE:  {test_mae:.2f} kWh")
print(f"  R¬≤:   {test_r2:.4f}")

# Feature importance
feature_importance = pd.DataFrame({
    'feature': xgb_model.feature_names_in_,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nüìä Top 15 Features:")
print(feature_importance.head(15).to_string(index=False))

# Check occupants importance
if 'occupants' in feature_importance['feature'].values:
    occ_imp = feature_importance[feature_importance['feature'] == 'occupants']['importance'].values[0]
    occ_rank = feature_importance[feature_importance['feature'] == 'occupants'].index[0] + 1
    print(f"\nüîç Occupants:")
    print(f"   - Importance: {occ_imp:.6f}")
    print(f"   - Rank: {occ_rank}/{len(feature_importance)}")
    
    if occ_imp > 0.01:
        print(f"   ‚úÖ T·ªët: Occupants c√≥ importance > 1%")
    else:
        print(f"   ‚ö†Ô∏è  V·∫´n th·∫•p: Occupants c√≥ importance < 1%")

# ============================================================================
# 5. SO S√ÅNH V·ªöI MODEL C≈®
# ============================================================================

print("\n" + "=" * 80)
print("SO S√ÅNH V·ªöI MODEL C≈®")
print("=" * 80)

print(f"\nüìä Model c≈© (ch·ªâ electricity_lag1):")
print(f"   - Test RMSE: ~48.55 kWh")
print(f"   - Test R¬≤: ~0.9394")

print(f"\nüìä Model m·ªõi (v·ªõi tuned hyperparameters):")
print(f"   - Test RMSE: {test_rmse:.2f} kWh")
print(f"   - Test R¬≤: {test_r2:.4f}")

improvement = (48.55 - test_rmse) / 48.55 * 100
print(f"\nüí° C·∫£i thi·ªán:")
print(f"   - RMSE gi·∫£m: {48.55 - test_rmse:.2f} kWh ({improvement:.1f}%)")

if test_rmse < 40:
    print(f"   ‚úÖ T·ªët: RMSE < 40 kWh")
elif test_rmse < 35:
    print(f"   ‚úÖ R·∫•t t·ªët: RMSE < 35 kWh")
else:
    print(f"   ‚ö†Ô∏è  V·∫´n cao: RMSE > 35 kWh")
    print(f"   üí° C√≥ th·ªÉ c·∫ßn gi·ªØ th√™m electricity_lag24")

# ============================================================================
# 6. L∆ØU MODEL (N·∫æU T·ªêT H∆†N)
# ============================================================================

if test_rmse < 48.55:
    print("\n" + "=" * 80)
    print("L∆ØU MODEL M·ªöI")
    print("=" * 80)
    
    # Create wrapper
    from src.inference import XGBoostWrapper
    wrapped_model = XGBoostWrapper(xgb_model, label_encoders, categorical_features)
    
    # Save model
    model_path = Path("output/models/xgboost_wrapped_dice.pkl")
    with open(model_path, 'wb') as f:
        pickle.dump(wrapped_model, f)
    print(f"‚úÖ ƒê√£ l∆∞u model: {model_path}")
    
    # Save encoders
    encoders_path = Path("output/models/label_encoders_dice.pkl")
    with open(encoders_path, 'wb') as f:
        pickle.dump(label_encoders, f)
    print(f"‚úÖ ƒê√£ l∆∞u encoders: {encoders_path}")
    
    # Save model info
    model_info = {
        'model_type': 'XGBoost',
        'training_date': datetime.now().isoformat(),
        'performance': {
            'train_rmse': float(train_rmse),
            'test_rmse': float(test_rmse),
            'train_mae': float(train_mae),
            'test_mae': float(test_mae),
            'train_r2': float(train_rmse),
            'test_r2': float(test_r2)
        },
        'hyperparameters': {
            'n_estimators': 500,
            'max_depth': 10,
            'learning_rate': 0.03,
            'subsample': 0.85,
            'colsample_bytree': 0.85,
            'min_child_weight': 2,
            'gamma': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0
        },
        'features': {
            'total': len(all_features) + len(categorical_features),
            'lag_features': [f for f in all_features if 'lag' in f]
        }
    }
    
    model_info_path = Path("output/models/model_info_dice.json")
    with open(model_info_path, 'w') as f:
        json.dump(model_info, f, indent=2)
    print(f"‚úÖ ƒê√£ l∆∞u model info: {model_info_path}")
    
    print(f"\nüí° Model m·ªõi ƒë√£ ƒë∆∞·ª£c l∆∞u!")
    print(f"   B·∫°n c√≥ th·ªÉ test l·∫°i v·ªõi: python src/test_model_behavior.py")
else:
    print(f"\n‚ö†Ô∏è  Model m·ªõi kh√¥ng t·ªët h∆°n model c≈©")
    print(f"   C√≥ th·ªÉ c·∫ßn:")
    print(f"   1. Gi·ªØ th√™m electricity_lag24 trong preprocessing")
    print(f"   2. Th·ª≠ c√°c hyperparameters kh√°c")
    print(f"   3. Feature engineering t·ªët h∆°n")

print("\n" + "=" * 80)
print("‚úÖ HO√ÄN T·∫§T!")
print("=" * 80)
